{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/TalkTrack/blob/main/analisis_seminario_colunga14_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqLzJIiyvbKW"
      },
      "source": [
        "## Transcribir texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDV8I2VvH0Rj",
        "outputId": "dcf03076-d776-4d07-c1ea-b81f8b636a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper\n",
            "  Cloning https://github.com/openai/whisper to /tmp/pip-req-build-1resqx30\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper /tmp/pip-req-build-1resqx30\n",
            "  Resolved https://github.com/openai/whisper to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20240930) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803669 sha256=65b956bb3bb70747ebf34ee9119da8378abd3924b898627857c2fff33afc9623\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s8hoybor/wheels/51/d4/af/7f87f0769301da47905e3197b7fca6593e58dc0ea98997f9d3\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "dU4QXyqsugX0",
        "outputId": "e6517a13-6976-4700-acc8-6f3f5ddb3b52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-23620e2a-8728-499f-9817-f465f725819f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-23620e2a-8728-499f-9817-f465f725819f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Conversatorio mesa educacion.m4a to Conversatorio mesa educacion.m4a\n",
            "Archivo de audio subido: Conversatorio mesa educacion.m4a\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Subir un archivo de audio desde tu computadora\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extraer el nombre del archivo subido\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "print(f\"Archivo de audio subido: {audio_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eoNzO-AumOy",
        "outputId": "f8b12ba5-b819-4339-dfcd-ba1356ae7faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.88G/2.88G [00:23<00:00, 134MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "# Cargar el modelo de Whisper (puedes usar 'base', 'small', 'medium', 'large' según tu necesidad)\n",
        "model = whisper.load_model(\"large\")\n",
        "\n",
        "# Transcribir el audio\n",
        "result = model.transcribe(audio_file)\n",
        "\n",
        "# Imprimir la transcripción\n",
        "transcription = result[\"text\"]\n",
        "print(\"Transcripción:\\n\", transcription)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doTYPnHvvPBO"
      },
      "outputs": [],
      "source": [
        "# Guardar la transcripción en un archivo .txt\n",
        "output_file = \"transcription.txt\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    f.write(transcription)\n",
        "\n",
        "print(f\"Transcripción guardada en: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aLf8EEkvefr"
      },
      "source": [
        "## Análisis de discurso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKP1B53ivhAU"
      },
      "outputs": [],
      "source": [
        "# Instalar las bibliotecas necesarias\n",
        "!pip install anthropic\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import os\n",
        "from anthropic import Client\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import output\n",
        "\n",
        "# Interfaz para ingresar la API key\n",
        "def request_anthropic_api_key():\n",
        "    input_html = \"\"\"\n",
        "    <div style=\"font-family: Arial; margin: 10px; padding: 10px; border: 1px solid #ddd; border-radius: 5px; width: 50%;\">\n",
        "        <h3 style=\"color: #007BFF;\">Proveer API Key</h3>\n",
        "        <p>Por favor, ingrese su API key de Anthropics en el siguiente campo:</p>\n",
        "        <input type=\"password\" id=\"anthropicApiKeyInput\" placeholder=\"Ingrese su API key aquí\"\n",
        "               style=\"width: 100%; padding: 8px; margin-bottom: 10px; border: 1px solid #ddd; border-radius: 3px;\">\n",
        "        <button onclick=\"storeApiKey()\"\n",
        "                style=\"padding: 10px 15px; background-color: #007BFF; color: white; border: none; border-radius: 3px; cursor: pointer;\">\n",
        "            Guardar API Key\n",
        "        </button>\n",
        "    </div>\n",
        "    <script>\n",
        "        function storeApiKey() {\n",
        "            const apiKey = document.getElementById('anthropicApiKeyInput').value;\n",
        "            if (apiKey) {\n",
        "                google.colab.kernel.invokeFunction('notebook.store_anthropic_api_key', [apiKey], {});\n",
        "                alert('API key almacenada correctamente.');\n",
        "            } else {\n",
        "                alert('Por favor, ingrese una API key válida.');\n",
        "            }\n",
        "        }\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(input_html))\n",
        "\n",
        "# Callback para almacenar la API key en la variable de entorno\n",
        "def store_anthropic_api_key(api_key):\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
        "    print(\"API key almacenada correctamente en la variable de entorno.\")\n",
        "\n",
        "# Registrar la función en Google Colab\n",
        "output.register_callback('notebook.store_anthropic_api_key', store_anthropic_api_key)\n",
        "\n",
        "# Solicitar la API key al usuario\n",
        "request_anthropic_api_key()\n",
        "\n",
        "# Crear un cliente para la API\n",
        "client = Client(api_key=store_anthropic_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "dp915KyMtuTc",
        "outputId": "f6729915-bb4a-43f6-9423-10a56b1f1d0e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"font-family: Arial; margin: 10px; padding: 10px; border: 1px solid #ddd; border-radius: 5px; width: 50%;\">\n",
              "        <h3 style=\"color: #007BFF;\">Proveer API Key</h3>\n",
              "        <p>Por favor, ingrese su API key de Anthropics en el siguiente campo:</p>\n",
              "        <input type=\"password\" id=\"anthropicApiKeyInput\" placeholder=\"Ingrese su API key aquí\"\n",
              "               style=\"width: 100%; padding: 8px; margin-bottom: 10px; border: 1px solid #ddd; border-radius: 3px;\">\n",
              "        <button onclick=\"storeApiKey()\"\n",
              "                style=\"padding: 10px 15px; background-color: #007BFF; color: white; border: none; border-radius: 3px; cursor: pointer;\">\n",
              "            Guardar API Key\n",
              "        </button>\n",
              "    </div>\n",
              "    <script>\n",
              "        function storeApiKey() {\n",
              "            const apiKey = document.getElementById('anthropicApiKeyInput').value;\n",
              "            if (apiKey) {\n",
              "                google.colab.kernel.invokeFunction('notebook.store_anthropic_api_key', [apiKey], {});\n",
              "                alert('API key almacenada correctamente.');\n",
              "            } else {\n",
              "                alert('Por favor, ingrese una API key válida.');\n",
              "            }\n",
              "        }\n",
              "    </script>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key almacenada correctamente en la variable de entorno.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "import os\n",
        "\n",
        "def analyze_discourse(text):\n",
        "    \"\"\"\n",
        "    Realiza un análisis de discurso usando la API de Anthropic.\n",
        "    \"\"\"\n",
        "    # Inicializar el cliente con la API key\n",
        "    client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "\n",
        "    # Definir el sistema y mensaje del usuario\n",
        "    system_prompt = \"You are tasked with analyzing the discourse of an audio transcript from a seminar on innovating and transforming the classroom.\"\n",
        "\n",
        "    user_message = f\"\"\"Here is the transcript of the audio:\n",
        "\n",
        "    <transcript>\n",
        "    {text}\n",
        "    </transcript>\n",
        "\n",
        "    Analyze the discourse of this seminar, focusing on the following areas:\n",
        "\n",
        "    1. Main themes and topics discussed\n",
        "    2. Innovative ideas or strategies presented\n",
        "    3. Structure and flow of the seminar\n",
        "    4. Speaker's communication style and effectiveness\n",
        "    5. Audience engagement (if applicable)\n",
        "    6. Use of examples or case studies\n",
        "    7. Potential impact on classroom transformation\n",
        "\n",
        "    Provide your analysis in a structured format with the following sections:\n",
        "\n",
        "    <analysis>\n",
        "    The analysis MUST BE IN SPANISH\n",
        "\n",
        "    <themes>\n",
        "    Discuss the main themes and topics of the seminar\n",
        "    </themes>\n",
        "\n",
        "    <innovation>\n",
        "    Highlight the innovative ideas or strategies presented for classroom transformation\n",
        "    </innovation>\n",
        "\n",
        "    <structure>\n",
        "    Analyze the structure and flow of the seminar\n",
        "    </structure>\n",
        "\n",
        "    <communication>\n",
        "    Evaluate the speaker's communication style and effectiveness\n",
        "    </communication>\n",
        "\n",
        "    <engagement>\n",
        "    Discuss any evidence of audience engagement or interaction\n",
        "    </engagement>\n",
        "\n",
        "    <examples>\n",
        "    Highlight any significant examples or case studies used\n",
        "    </examples>\n",
        "\n",
        "    <impact>\n",
        "    Assess the potential impact of the ideas presented on classroom transformation\n",
        "    </impact>\n",
        "    </analysis>\n",
        "\n",
        "    After completing the structured analysis, provide a concise summary:\n",
        "\n",
        "    <summary>\n",
        "    Summarize the key points of your analysis and the overall effectiveness of the seminar: The text MUST BE IN SPANISH\n",
        "    </summary>\"\"\"\n",
        "\n",
        "    # Llamada a la API usando el nuevo formato de Messages\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-sonnet-20240229\",\n",
        "        max_tokens=4096,\n",
        "        system=system_prompt,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extraer el contenido de la respuesta\n",
        "    analysis_result = response.content[0].text\n",
        "    return analysis_result\n",
        "\n",
        "# Cargar el texto de la transcripción\n",
        "with open(\"transcription.txt\", \"r\") as f:\n",
        "    transcription_text = f.read()\n",
        "\n",
        "# Realizar el análisis\n",
        "analysis = analyze_discourse(transcription_text)\n",
        "\n",
        "# Mostrar el análisis\n",
        "print(\"Análisis de Discurso:\\n\", analysis)\n",
        "\n",
        "# Guardar el análisis en un archivo\n",
        "with open(\"discourse_analysis.txt\", \"w\") as f:\n",
        "    f.write(analysis)\n",
        "\n",
        "print(\"El análisis ha sido guardado en 'discourse_analysis.txt'.\")"
      ],
      "metadata": {
        "id": "fOlX-pAj9NLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "import os\n",
        "\n",
        "def analyze_discourse(text):\n",
        "    \"\"\"\n",
        "    Realiza un análisis de discurso usando la API de Anthropic.\n",
        "    \"\"\"\n",
        "    # Definir un prompt ajustado al contexto del análisis\n",
        "    prompt = f\"\"\"\n",
        "    \\n\\nHuman: You are tasked with analyzing the discourse of an audio transcript from a seminar on innovating and transforming the classroom. Your goal is to provide insights into the content, structure, and key themes of the seminar.\n",
        "\n",
        "    Here is the transcript of the audio:\n",
        "\n",
        "    <transcript>\n",
        "    {text}\n",
        "    </transcript>\n",
        "\n",
        "    Analyze the discourse of this seminar, focusing on the following areas:\n",
        "\n",
        "    1. Main themes and topics discussed\n",
        "    2. Innovative ideas or strategies presented\n",
        "    3. Structure and flow of the seminar\n",
        "    4. Speaker's communication style and effectiveness\n",
        "    5. Audience engagement (if applicable)\n",
        "    6. Use of examples or case studies\n",
        "    7. Potential impact on classroom transformation\n",
        "\n",
        "    Provide your analysis in a structured format with the following sections:\n",
        "\n",
        "    <analysis>\n",
        "    <themes>\n",
        "    Discuss the main themes and topics of the seminar\n",
        "    </themes>\n",
        "\n",
        "    <innovation>\n",
        "    Highlight the innovative ideas or strategies presented for classroom transformation\n",
        "    </innovation>\n",
        "\n",
        "    <structure>\n",
        "    Analyze the structure and flow of the seminar\n",
        "    </structure>\n",
        "\n",
        "    <communication>\n",
        "    Evaluate the speaker's communication style and effectiveness\n",
        "    </communication>\n",
        "\n",
        "    <engagement>\n",
        "    Discuss any evidence of audience engagement or interaction\n",
        "    </engagement>\n",
        "\n",
        "    <examples>\n",
        "    Highlight any significant examples or case studies used\n",
        "    </examples>\n",
        "\n",
        "    <impact>\n",
        "    Assess the potential impact of the ideas presented on classroom transformation\n",
        "    </impact>\n",
        "    </analysis>\n",
        "\n",
        "    In each section, use specific evidence from the transcript to support your analysis. Quote relevant parts of the transcript when appropriate, using quotation marks.\n",
        "\n",
        "    After completing the structured analysis, provide a concise summary of your findings in a separate section:\n",
        "\n",
        "    <summary>\n",
        "    Summarize the key points of your analysis and the overall effectiveness of the seminar in addressing innovation and transformation in the classroom\n",
        "    </summary>\n",
        "\n",
        "    Remember to base your analysis solely on the content provided in the transcript. If certain aspects are not addressed in the transcript, you may note their absence in your analysis.\n",
        "\n",
        "    \\n\\nAssistant:\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializar el cliente con la API key\n",
        "    client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "\n",
        "    # Llamada a la API\n",
        "\n",
        "    response = client.completions.create(\n",
        "    prompt=prompt,\n",
        "    model=\"claude-3-5-sonnet-20241022\",\n",
        "    max_tokens_to_sample=4096,\n",
        "    stop_sequences=[\"\\n\\n\"]\n",
        "    )\n",
        "    print(\"Respuesta de la API:\\n\", response)\n",
        "\n",
        "\n",
        "# Cargar el texto de la transcripción\n",
        "with open(\"transcription.txt\", \"r\") as f:\n",
        "    transcription_text = f.read()\n",
        "\n",
        "# Realizar el análisis\n",
        "analysis = analyze_discourse(transcription_text)\n",
        "\n",
        "# Mostrar el análisis\n",
        "print(\"Análisis de Discurso:\\n\", analysis)\n",
        "\n",
        "# Guardar el análisis en un archivo\n",
        "with open(\"discourse_analysis.txt\", \"w\") as f:\n",
        "    f.write(analysis)\n",
        "\n",
        "print(\"El análisis ha sido guardado en 'discourse_analysis.txt'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "JXHj4xxyvtiy",
        "outputId": "af4c9327-2d7a-43fe-b9cf-316ffdd1caf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': '\"claude-3-5-sonnet-20241022\" is not supported on this API. Please use the Messages API instead.'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d54666808ad8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Realizar el análisis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_discourse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Mostrar el análisis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d54666808ad8>\u001b[0m in \u001b[0;36manalyze_discourse\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Llamada a la API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     response = client.completions.create(\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-3-5-sonnet-20241022\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens_to_sample, model, prompt, metadata, stop_sequences, stream, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;34m\"/v1/complete\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         )\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': '\"claude-3-5-sonnet-20241022\" is not supported on this API. Please use the Messages API instead.'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DcZTse55xp30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "4d96e45e-4a99-4e96-c748-d2393f99dce9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt must start with \"\\n\\nHuman:\" turn after an optional system prompt'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1575f7026099>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Realizar el análisis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_discourse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Mostrar el análisis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1575f7026099>\u001b[0m in \u001b[0;36manalyze_discourse\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Llamada a la API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     response = client.completions.create(\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-3-5-sonnet-20241022\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens_to_sample, model, prompt, metadata, stop_sequences, stream, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;34m\"/v1/complete\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         )\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt must start with \"\\n\\nHuman:\" turn after an optional system prompt'}}"
          ]
        }
      ],
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "def analyze_discourse(text):\n",
        "    \"\"\"\n",
        "    Realiza un análisis de discurso usando la API de Anthropic.\n",
        "    \"\"\"\n",
        "    # Definir un prompt ajustado al contexto del análisis\n",
        "    prompt = f\"\"\"\n",
        "    You are tasked with analyzing the discourse of an audio transcript from a seminar on innovating and transforming the classroom. Your goal is to provide insights into the content, structure, and key themes of the seminar.\n",
        "\n",
        "    Here is the transcript of the audio:\n",
        "\n",
        "    <transcript>\n",
        "    {text}\n",
        "    </transcript>\n",
        "\n",
        "    Analyze the discourse of this seminar, focusing on the following areas:\n",
        "\n",
        "    1. Main themes and topics discussed\n",
        "    2. Innovative ideas or strategies presented\n",
        "    3. Structure and flow of the seminar\n",
        "    4. Speaker's communication style and effectiveness\n",
        "    5. Audience engagement (if applicable)\n",
        "    6. Use of examples or case studies\n",
        "    7. Potential impact on classroom transformation\n",
        "\n",
        "    Provide your analysis in a structured format with the following sections:\n",
        "\n",
        "    <analysis>\n",
        "    <themes>\n",
        "    Discuss the main themes and topics of the seminar\n",
        "    </themes>\n",
        "\n",
        "    <innovation>\n",
        "    Highlight the innovative ideas or strategies presented for classroom transformation\n",
        "    </innovation>\n",
        "\n",
        "    <structure>\n",
        "    Analyze the structure and flow of the seminar\n",
        "    </structure>\n",
        "\n",
        "    <communication>\n",
        "    Evaluate the speaker's communication style and effectiveness\n",
        "    </communication>\n",
        "\n",
        "    <engagement>\n",
        "    Discuss any evidence of audience engagement or interaction\n",
        "    </engagement>\n",
        "\n",
        "    <examples>\n",
        "    Highlight any significant examples or case studies used\n",
        "    </examples>\n",
        "\n",
        "    <impact>\n",
        "    Assess the potential impact of the ideas presented on classroom transformation\n",
        "    </impact>\n",
        "    </analysis>\n",
        "\n",
        "    In each section, use specific evidence from the transcript to support your analysis. Quote relevant parts of the transcript when appropriate, using quotation marks.\n",
        "\n",
        "    After completing the structured analysis, provide a concise summary of your findings in a separate section:\n",
        "\n",
        "    <summary>\n",
        "    Summarize the key points of your analysis and the overall effectiveness of the seminar in addressing innovation and transformation in the classroom\n",
        "    </summary>\n",
        "\n",
        "    Remember to base your analysis solely on the content provided in the transcript. If certain aspects are not addressed in the transcript, you may note their absence in your analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializar el cliente con la API key\n",
        "    client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "\n",
        "    # Llamada a la API\n",
        "    response = client.completions.create(\n",
        "        prompt=prompt,\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        max_tokens_to_sample=4096,\n",
        "        stop_sequences=[\"\\n\\n\"]\n",
        "    )\n",
        "    return response[\"completion\"]\n",
        "\n",
        "# Cargar el texto de la transcripción\n",
        "with open(\"transcription.txt\", \"r\") as f:\n",
        "    transcription_text = f.read()\n",
        "\n",
        "# Realizar el análisis\n",
        "analysis = analyze_discourse(transcription_text)\n",
        "\n",
        "# Mostrar el análisis\n",
        "print(\"Análisis de Discurso:\\n\", analysis)\n",
        "\n",
        "# Guardar el análisis en un archivo\n",
        "with open(\"discourse_analysis.txt\", \"w\") as f:\n",
        "    f.write(analysis)\n",
        "\n",
        "print(\"El análisis ha sido guardado en 'discourse_analysis.txt'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhqa-HdByBNF"
      },
      "source": [
        "### Nubes de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M6YeZNqyDdr"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih8I35ubyALT"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_wordcloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Nube de Palabras del Discurso\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Generar la nube de palabras a partir del texto de la transcripción\n",
        "generate_wordcloud(transcription_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL2eDnMCyFdX"
      },
      "source": [
        "### Análisis de Sentimientos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoWT_ZayyK8L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RGy7debyJY2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Crear pipeline de análisis de sentimientos\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Dividir el texto en párrafos y analizar cada uno\n",
        "sentiments = sentiment_analyzer(transcription_text.split(\"\\n\"))\n",
        "\n",
        "# Mostrar los resultados\n",
        "for idx, sentiment in enumerate(sentiments):\n",
        "    print(f\"Párrafo {idx + 1}: {sentiment}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hstnTbaKyNjz"
      },
      "source": [
        "### Gráficos para Análisis de Temas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1p_Dv99yRKX"
      },
      "outputs": [],
      "source": [
        "!pip install gensim pyLDAvis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whGZjRGzyOwp"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "# Preprocesar el texto\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "# Tokenizar la transcripción\n",
        "tokens = preprocess_text(transcription_text)\n",
        "\n",
        "# Crear un diccionario y un corpus\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "corpus = [dictionary.doc2bow(tokens)]\n",
        "\n",
        "# Crear modelo LDA\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)\n",
        "\n",
        "# Visualizar temas\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(lda_vis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmQMglvOyV8h"
      },
      "source": [
        "### Visualización del Impacto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpI6t3n3yX1B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Contar las palabras más frecuentes\n",
        "word_counts = Counter(tokens)\n",
        "most_common_words = word_counts.most_common(10)\n",
        "\n",
        "# Gráfico de barras\n",
        "words, counts = zip(*most_common_words)\n",
        "plt.bar(words, counts)\n",
        "plt.title(\"Palabras más frecuentes\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A-Fm5Y0ygCZ"
      },
      "source": [
        "### Resumen Visual\n",
        "\n",
        "Diagrama de Sankey\n",
        "\n",
        "Un diagrama de Sankey es excelente para mostrar relaciones entre temas principales y secundarios o el flujo lógico de una narrativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57Zg4ygUyk9y"
      },
      "outputs": [],
      "source": [
        "!pip install plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD74qVAcyh4p"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def create_sankey_chart(data):\n",
        "    \"\"\"\n",
        "    Crea un diagrama de Sankey para representar visualmente el resumen del discurso.\n",
        "    \"\"\"\n",
        "    # Definir los nodos (temas principales y secundarios)\n",
        "    nodes = list(set([item for sublist in data for item in sublist]))\n",
        "    node_indices = {node: i for i, node in enumerate(nodes)}\n",
        "\n",
        "    # Definir las conexiones entre los nodos\n",
        "    sources = [node_indices[source] for source, target, value in data]\n",
        "    targets = [node_indices[target] for source, target, value in data]\n",
        "    values = [value for source, target, value in data]\n",
        "\n",
        "    # Crear el diagrama de Sankey\n",
        "    fig = go.Figure(go.Sankey(\n",
        "        node=dict(\n",
        "            pad=15,\n",
        "            thickness=20,\n",
        "            line=dict(color=\"black\", width=0.5),\n",
        "            label=nodes\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=sources,\n",
        "            target=targets,\n",
        "            value=values\n",
        "        )\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(title_text=\"Resumen Visual del Discurso\", font_size=10)\n",
        "    fig.show()\n",
        "\n",
        "# Datos de ejemplo (modifica según el análisis del discurso)\n",
        "sankey_data = [\n",
        "    (\"Innovación\", \"Uso de tecnología\", 5),\n",
        "    (\"Innovación\", \"Ejemplos prácticos\", 3),\n",
        "    (\"Uso de tecnología\", \"Herramientas digitales\", 2),\n",
        "    (\"Uso de tecnología\", \"Aplicaciones móviles\", 3),\n",
        "    (\"Ejemplos prácticos\", \"Casos reales\", 4),\n",
        "    (\"Casos reales\", \"Impacto en el aula\", 6),\n",
        "]\n",
        "\n",
        "# Crear el diagrama\n",
        "create_sankey_chart(sankey_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiuPQuyfytcB"
      },
      "source": [
        "Mapa Mental\n",
        "\n",
        "Si prefieres un mapa mental, podemos usar networkx y matplotlib para generar un gráfico de nodos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjKTpsw4yxHM"
      },
      "outputs": [],
      "source": [
        "!pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpuxeeEEyuOd"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_mind_map(data):\n",
        "    \"\"\"\n",
        "    Crea un mapa mental para representar visualmente el discurso.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Agregar nodos y relaciones\n",
        "    for source, target, _ in data:\n",
        "        G.add_edge(source, target)\n",
        "\n",
        "    # Crear el diseño del gráfico\n",
        "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    nx.draw_networkx(\n",
        "        G, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\",\n",
        "        font_size=10, font_weight=\"bold\", node_size=3000\n",
        "    )\n",
        "    plt.title(\"Mapa Mental del Discurso\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Usar los mismos datos de ejemplo\n",
        "create_mind_map(sankey_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCz4eQd4y68P"
      },
      "source": [
        "### Grafo con networkx y matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5pNxrXLy99x"
      },
      "outputs": [],
      "source": [
        "!pip install networkx matplotlib\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_graph(data):\n",
        "    \"\"\"\n",
        "    Crea un grafo simple para representar las conexiones entre temas.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Agregar nodos y relaciones desde los datos\n",
        "    for source, target, weight in data:\n",
        "        G.add_edge(source, target, weight=weight)\n",
        "\n",
        "    # Crear un diseño para el grafo\n",
        "    pos = nx.spring_layout(G, k=0.5)\n",
        "\n",
        "    # Dibujar el grafo\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    nx.draw(\n",
        "        G, pos, with_labels=True, node_color=\"skyblue\", edge_color=\"gray\",\n",
        "        font_size=10, font_weight=\"bold\", node_size=3000\n",
        "    )\n",
        "\n",
        "    # Agregar etiquetas para los pesos de las conexiones\n",
        "    edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)\n",
        "\n",
        "    plt.title(\"Grafo de Temas del Discurso\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Datos de ejemplo (tema principal, tema relacionado, peso de conexión)\n",
        "graph_data = [\n",
        "    (\"Innovación\", \"Uso de tecnología\", 5),\n",
        "    (\"Innovación\", \"Ejemplos prácticos\", 3),\n",
        "    (\"Uso de tecnología\", \"Herramientas digitales\", 4),\n",
        "    (\"Ejemplos prácticos\", \"Casos reales\", 2),\n",
        "    (\"Casos reales\", \"Impacto en el aula\", 6)\n",
        "]\n",
        "\n",
        "# Crear el grafo\n",
        "create_graph(graph_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLBvdyTIy-au"
      },
      "source": [
        "### Grafo Interactivo con pyvis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xMY-i8_zDaz"
      },
      "outputs": [],
      "source": [
        "!pip install pyvis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRljz8DCzA3T"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "def create_interactive_graph(data):\n",
        "    \"\"\"\n",
        "    Crea un grafo interactivo usando pyvis.\n",
        "    \"\"\"\n",
        "    net = Network(notebook=True, height=\"750px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
        "\n",
        "    # Agregar nodos y relaciones desde los datos\n",
        "    for source, target, weight in data:\n",
        "        net.add_node(source, title=source)\n",
        "        net.add_node(target, title=target)\n",
        "        net.add_edge(source, target, value=weight)\n",
        "\n",
        "    # Configurar opciones del grafo\n",
        "    net.set_options('''\n",
        "    var options = {\n",
        "      \"nodes\": {\n",
        "        \"color\": {\n",
        "          \"border\": \"rgba(0,0,0,1)\",\n",
        "          \"background\": \"rgba(135,206,250,1)\"\n",
        "        }\n",
        "      },\n",
        "      \"edges\": {\n",
        "        \"color\": {\n",
        "          \"inherit\": true\n",
        "        },\n",
        "        \"smooth\": {\n",
        "          \"type\": \"dynamic\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "    # Mostrar el grafo\n",
        "    net.show(\"graph.html\")\n",
        "\n",
        "# Crear el grafo interactivo\n",
        "create_interactive_graph(graph_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usXq9RqKzPYG"
      },
      "source": [
        "## Datos Dinámicos:\n",
        "\n",
        "Usa un modelo LLM para extraer automáticamente temas principales, relaciones, y pesos del discurso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWso-WoLzQVH"
      },
      "outputs": [],
      "source": [
        "from anthropic import Client\n",
        "import os\n",
        "\n",
        "# Cliente de Anthropic\n",
        "client = Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "\n",
        "def extract_topics_and_relations(text):\n",
        "    \"\"\"\n",
        "    Extrae temas principales, relaciones y pesos del discurso usando un LLM.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert in discourse analysis. Based on the following transcript, extract the main topics, their subtopics,\n",
        "    and the relationships between them. Include a relevance score (1-10) for each relationship.\n",
        "\n",
        "    Transcript:\n",
        "    {text}\n",
        "\n",
        "    Provide the output in the following format:\n",
        "    [\n",
        "        (\"Main Topic\", \"Related Topic\", Relevance Score),\n",
        "        ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    response = client.completion(\n",
        "        prompt=prompt,\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        max_tokens_to_sample=1000,\n",
        "        stop_sequences=[\"\\n\\n\"]\n",
        "    )\n",
        "    return eval(response[\"completion\"])\n",
        "\n",
        "# Usar la función en la transcripción\n",
        "with open(\"transcription.txt\", \"r\") as f:\n",
        "    transcription_text = f.read()\n",
        "\n",
        "relations_data = extract_topics_and_relations(transcription_text)\n",
        "print(\"Relaciones extraídas:\\n\", relations_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXnPE5EUzT8p"
      },
      "source": [
        "## Crear un Grafo Dinámico con los Datos Extraídos\n",
        "Ahora que tenemos los datos generados, podemos usarlos para construir un grafo dinámico.\n",
        "\n",
        "Grafo Dinámico con pyvis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVRhto_DzW8h"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "def create_dynamic_graph(data):\n",
        "    \"\"\"\n",
        "    Crea un grafo interactivo usando datos generados por un modelo LLM.\n",
        "    \"\"\"\n",
        "    net = Network(notebook=True, height=\"750px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
        "\n",
        "    # Agregar nodos y relaciones\n",
        "    for source, target, weight in data:\n",
        "        net.add_node(source, title=source)\n",
        "        net.add_node(target, title=target)\n",
        "        net.add_edge(source, target, value=weight)\n",
        "\n",
        "    # Configurar opciones del grafo\n",
        "    net.set_options('''\n",
        "    var options = {\n",
        "      \"nodes\": {\n",
        "        \"color\": {\n",
        "          \"border\": \"rgba(0,0,0,1)\",\n",
        "          \"background\": \"rgba(135,206,250,1)\"\n",
        "        }\n",
        "      },\n",
        "      \"edges\": {\n",
        "        \"color\": {\n",
        "          \"inherit\": true\n",
        "        },\n",
        "        \"smooth\": {\n",
        "          \"type\": \"dynamic\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "    # Mostrar el grafo\n",
        "    net.show(\"dynamic_graph.html\")\n",
        "\n",
        "# Crear el grafo dinámico con los datos extraídos\n",
        "create_dynamic_graph(relations_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwJZi9Adzipx"
      },
      "source": [
        "### Nube de Palabras: Resalta Términos Frecuentes\n",
        "Crea una nube de palabras para identificar rápidamente las palabras clave más utilizadas en el discurso.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii2KPlNBzkDJ"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud matplotlib\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_wordcloud_from_topics(data):\n",
        "    \"\"\"\n",
        "    Genera una nube de palabras basada en los temas y sus pesos.\n",
        "    \"\"\"\n",
        "    # Crear un diccionario con temas y pesos\n",
        "    topic_weights = {source: weight for source, _, weight in data}\n",
        "    topic_weights.update({target: weight for _, target, weight in data})\n",
        "\n",
        "    # Generar la nube de palabras\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(topic_weights)\n",
        "\n",
        "    # Mostrar la nube de palabras\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Nube de Palabras Basada en Temas\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Generar la nube de palabras\n",
        "generate_wordcloud_from_topics(relations_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bVU5mAYznMk"
      },
      "source": [
        "## Mapa de Calor: Relación Entre Temas\n",
        "Un mapa de calor (heatmap) muestra cómo de fuertes son las relaciones entre temas principales y secundarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fa2i9_2zoIz"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_heatmap(data):\n",
        "    \"\"\"\n",
        "    Genera un mapa de calor para visualizar las relaciones entre temas.\n",
        "    \"\"\"\n",
        "    # Crear un DataFrame a partir de los datos\n",
        "    df = pd.DataFrame(data, columns=[\"Tema Principal\", \"Tema Relacionado\", \"Peso\"])\n",
        "\n",
        "    # Pivotar la tabla para crear la matriz\n",
        "    matrix = df.pivot(\"Tema Principal\", \"Tema Relacionado\", \"Peso\")\n",
        "\n",
        "    # Crear el mapa de calor\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(matrix, annot=True, cmap=\"Blues\", fmt=\".1f\")\n",
        "    plt.title(\"Mapa de Calor de Relaciones entre Temas\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Generar el mapa de calor\n",
        "create_heatmap(relations_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjW7WbeszrvZ"
      },
      "source": [
        "## Análisis Temporal: Evolución de Temas\n",
        "Si tu discurso está dividido en segmentos temporales (e.g., minutos o secciones), puedes analizar cómo evolucionan los temas a lo largo del tiempo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EeBbBNFzrV9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_topic_evolution(data, timestamps):\n",
        "    \"\"\"\n",
        "    Visualiza la evolución de los temas principales en el tiempo.\n",
        "    \"\"\"\n",
        "    # Crear un diccionario para contar la aparición de temas por timestamp\n",
        "    topic_evolution = {topic: [0] * len(timestamps) for topic, _, _ in data}\n",
        "\n",
        "    # Llenar el diccionario con datos\n",
        "    for i, (topic, _, weight) in enumerate(data):\n",
        "        topic_evolution[topic][i % len(timestamps)] += weight\n",
        "\n",
        "    # Graficar la evolución\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for topic, values in topic_evolution.items():\n",
        "        plt.plot(timestamps, values, label=topic)\n",
        "\n",
        "    plt.title(\"Evolución de Temas en el Tiempo\", fontsize=16)\n",
        "    plt.xlabel(\"Tiempo\")\n",
        "    plt.ylabel(\"Relevancia\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Ejemplo de timestamps (ajusta según tu análisis)\n",
        "timestamps = [\"Inicio\", \"Mitad\", \"Final\"]\n",
        "plot_topic_evolution(relations_data, timestamps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXW9Nk_hzvZA"
      },
      "source": [
        "## Árbol Jerárquico: Relación Principal-Secundaria\n",
        "Visualiza los temas en forma de árbol jerárquico, con los temas principales como nodos padres y los secundarios como hijos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cckgI31RzxCH"
      },
      "outputs": [],
      "source": [
        "!pip install plotly\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def create_tree_map(data):\n",
        "    \"\"\"\n",
        "    Genera un árbol jerárquico para mostrar relaciones entre temas.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    parents = []\n",
        "\n",
        "    for source, target, weight in data:\n",
        "        labels.append(target)\n",
        "        parents.append(source)\n",
        "\n",
        "    fig = go.Figure(go.Treemap(\n",
        "        labels=labels + [\"Total\"],\n",
        "        parents=parents + [\"\"],\n",
        "        values=[weight for _, _, weight in data] + [sum(weight for _, _, weight in data)]\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(title=\"Árbol Jerárquico de Temas\")\n",
        "    fig.show()\n",
        "\n",
        "# Crear el árbol jerárquico\n",
        "create_tree_map(relations_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf_ovIp5zzkn"
      },
      "source": [
        "## Predicciones y Análisis Contextual:\n",
        "Usa un modelo de lenguaje para:\n",
        "\n",
        "Generar predicciones sobre el impacto de los temas en el contexto del discurso (e.g., \"¿Cómo impactará X en la educación?\").\n",
        "\n",
        "Identificar conexiones ocultas entre temas que no son evidentes en el análisis inicial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGFlGIqmz2rp"
      },
      "outputs": [],
      "source": [
        "def generate_additional_insights(text):\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following topics and relationships, provide additional insights or predictions:\n",
        "    {relations_data}\n",
        "\n",
        "    Insights should include:\n",
        "    1. Hidden connections between topics.\n",
        "    2. Future trends related to these topics.\n",
        "    3. Recommendations for further exploration.\n",
        "\n",
        "    Transcript:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    response = client.completion(\n",
        "        prompt=prompt,\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        max_tokens_to_sample=1000\n",
        "    )\n",
        "    return response[\"completion\"]\n",
        "\n",
        "additional_insights = generate_additional_insights(transcription_text)\n",
        "print(\"Insights Adicionales:\\n\", additional_insights)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM1UeHWXtvnGthGbqOuwnQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}